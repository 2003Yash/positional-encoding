{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXaCc79vsZb7vslf3wu+b1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/positional-encoding/blob/main/Positional_Encoding_for_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Positional encoding is a technique used in models like Transformers to provide positional information to the model by adding position-dependent signals to word embeddings, allowing the model to incorporate the order of words in the input sequence ***"
      ],
      "metadata": {
        "id": "vMMI6xS3Glms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we perform pos encoding with sine ans cosine fucntion and if i is even we use sine and odd i we use cosine"
      ],
      "metadata": {
        "id": "gRMfH6ZWA7wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#this is used in research paper but nowadays we are training the pos encoding along with model parameters\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model # word embedding dimension, usually it's no.of possibe words in out dictionary it's just one-hot embedding of each word\n",
        "\n",
        "# formula : https://towardsdatascience.com/understanding-positional-encoding-in-transformers-dc6bafc021ab\n",
        "\n",
        "    def call(self):\n",
        "        even_i = tf.range(0, self.d_model, 2, dtype=tf.float32) # 0,d_model,2 means 0 to till d_model row but each time instead of i++ we use i+2\n",
        "        denominator = tf.pow(10000.0, even_i/self.d_model) # we costamized formula to use same denominator for even and ood since it found to work best\n",
        "        position = tf.range(self.max_sequence_length, dtype=tf.float32)[:, tf.newaxis]\n",
        "        even_PE = tf.sin(position / denominator) # pos encoding on even\n",
        "        odd_PE = tf.cos(position / denominator) # pos encoding on odd\n",
        "        # we add this pos encoding to word embeeding vector to give it a unique embedding based on it's position\n",
        "        stacked = tf.stack([even_PE, odd_PE], axis=2) # axis is 2 i.e rows and it concatenates tensors into a new tensor\n",
        "        PE = tf.reshape(stacked, (self.max_sequence_length, self.d_model))\n",
        "        return PE"
      ],
      "metadata": {
        "id": "8VH-5sAd_PEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding(d_model=2, max_sequence_length=10) #usually d_model is 512 but for convinence we used 6\n",
        "pe.call()\n",
        "# pos encoding is creating unique tensor values for each rom in order to preserve the order of words\n",
        "#we just generated pos encoding tensor later we will add it to word embedding tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2KLrB6V_pGy",
        "outputId": "51ff7b12-a4fe-48d7-b974-8786927a4988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\n",
              "array([[ 0.        ,  1.        ],\n",
              "       [ 0.84147096,  0.5403023 ],\n",
              "       [ 0.9092974 , -0.4161468 ],\n",
              "       [ 0.14112   , -0.9899925 ],\n",
              "       [-0.7568025 , -0.6536436 ],\n",
              "       [-0.9589243 ,  0.28366217],\n",
              "       [-0.2794155 ,  0.96017027],\n",
              "       [ 0.6569866 ,  0.75390226],\n",
              "       [ 0.98935825, -0.14550003],\n",
              "       [ 0.4121185 , -0.91113025]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input (10,512) -> input + pos encoding (10,512) -> get q,k,v then data = (10, 1536) -> multi head attention"
      ],
      "metadata": {
        "id": "llyrCfweFFNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Explanation:\n",
        "# https://www.youtube.com/watch?v=T3OT8kqoqjc\n",
        "\n",
        "similar to binary encoding i.e.., all numerics are have unique binary representation similarly we us sine and cosine functions\n",
        "so each row has 2x more period of function so that they will always be unique just like binary each numeric has 2x bigger binary function than it's previous digit.\n",
        "based on the value we can estimate the distance between tokens and also estimate tokens position while not randomly assigning values and also uniquely assigning values using a function\n"
      ],
      "metadata": {
        "id": "I-53NKD_wus5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}